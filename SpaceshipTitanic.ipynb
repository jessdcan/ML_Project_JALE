{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\\sample_submission.csv\n",
      "input\\test.csv\n",
      "input\\train.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8693"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "train_df = pd.read_csv('input/test.csv')\n",
    "test_df = pd.read_csv('input/train.csv')\n",
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def apply_ml_model(model, X, y, test_size=0.2, random_state=None):\n",
    "    \"\"\"\n",
    "    Apply a machine learning model on the given data and calculate accuracy.\n",
    "\n",
    "    Parameters:\n",
    "        model: The machine learning model to be applied.\n",
    "        X: The features of the dataset.\n",
    "        y: The target variable of the dataset.\n",
    "        test_size: The proportion of the dataset to include in the test split.\n",
    "        random_state: Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        accuracy: Accuracy of the model on the test set.\n",
    "    \"\"\"\n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load your data here\n",
    "# Assuming you have features in X and target variable in y\n",
    "SEED = 42\n",
    "# Define Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=SEED)\n",
    "\n",
    "# Apply Random Forest model\n",
    "rf_accuracy = apply_ml_model(rf_model, X, y)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "\n",
    "# Define XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=SEED)\n",
    "\n",
    "# Apply XGBoost model\n",
    "xgb_accuracy = apply_ml_model(xgb_model, X, y)\n",
    "print(\"XGBoost Accuracy:\", xgb_accuracy)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define Logistic Regression model\n",
    "logreg_model = LogisticRegression()\n",
    "\n",
    "# Apply Logistic Regression model\n",
    "logreg_accuracy = apply_ml_model(logreg_model, X, y)\n",
    "print(\"Logistic Regression Accuracy:\", logreg_accuracy)\n",
    "\n",
    "# Define K-Nearest Neighbors model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Apply K-Nearest Neighbors model\n",
    "knn_accuracy = apply_ml_model(knn_model, X, y)\n",
    "print(\"K-Nearest Neighbors Accuracy:\", knn_accuracy)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Define Naive Bayes model\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "# Apply Naive Bayes model\n",
    "nb_accuracy = apply_ml_model(nb_model, X, y)\n",
    "print(\"Naive Bayes Accuracy:\", nb_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "Exploratory data analysis (EDA) is performed on the datasat to investigate features. Firstly the target feature is plot as pie chart to illustrate the distrubtion of passengers that were transported or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
